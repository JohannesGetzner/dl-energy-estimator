{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.data_utils import preprocess_and_normalize_energy_data\n",
    "from estimator.architecture_parser import parse_architecture\n",
    "from run_estimation import compute_energy_estimate\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Customn Definitions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dictionary of architecture names and their PyTorch instances\n",
    "architectures_dict = {\n",
    "    'alexnet': models.alexnet(weights=None),\n",
    "    'vgg13': models.vgg13(weights=None),\n",
    "    'vgg11': models.vgg11(weights=None),\n",
    "    'vgg16': models.vgg16(weights=None),\n",
    "    'resnet18': models.resnet18(weights=None),\n",
    "    'resnet34': models.resnet34(weights=None),\n",
    "    'resnet50': models.resnet50(weights=None)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "## Load Data & Normalize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load\n",
    "df_unnormalized = pd.read_csv('../data/architectures-energies-parsed.csv')\n",
    "# normalize\n",
    "df = preprocess_and_normalize_energy_data(df_unnormalized,['module','batch_size','architecture','layer_idx'], aggregate=True, verbose=True)\n",
    "print(\"Measured models:\", df.architecture.unique())\n",
    "df.head(n=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove bad measurements with high devations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute % contribution\n",
    "architecture_wise_deviations = pd.DataFrame(columns=['architecture', 'batch_size', 'percent_deviation', 'total_agg_energy','measured_energy'])\n",
    "for a in df.architecture.unique():\n",
    "    # subset data by architecture\n",
    "    sub = df.loc[df.architecture == a]\n",
    "    for b in sub.batch_size.unique():\n",
    "        # subset data by batch_size\n",
    "        sub_b = sub.loc[sub.batch_size == b]\n",
    "        # get energy from complete architecture run\n",
    "        total_measured_energy = sub_b.loc[sub_b.layer_idx == 0].cpu_energy.item()\n",
    "        # subset only data from individual modules of architecture\n",
    "        sub_b_modules = sub_b.loc[sub_b.layer_idx != 0]\n",
    "        # compute empirical total energy from complete architecture run by summing up modules\n",
    "        total_agg_energy = sum(sub_b_modules.groupby(['architecture', 'batch_size', 'module'])['cpu_energy'].sum().reset_index().cpu_energy)\n",
    "        new_row = {'architecture':a, 'batch_size':b, 'percent_deviation':(total_agg_energy - total_measured_energy) / total_agg_energy, 'total_agg_energy':total_agg_energy,'measured_energy':total_measured_energy}\n",
    "        architecture_wise_deviations = pd.concat([architecture_wise_deviations, pd.DataFrame(new_row,index=[0])], ignore_index=True)\n",
    "# print avg energies; compare energy when running the full architecture vs summing the energy over the individual modules\n",
    "agg = architecture_wise_deviations.groupby(['architecture']).mean()\n",
    "agg.columns = [f\"mean_{col_name}\" for col_name in agg.columns]\n",
    "agg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check for deviations that are too big\n",
    "blacklist = []\n",
    "data = df.copy()\n",
    "for idx, row in architecture_wise_deviations.iterrows():\n",
    "    if abs(row.percent_deviation) > 0.9:\n",
    "        blacklist.append({'architecture': row.architecture, 'batch_size': row.batch_size})\n",
    "if len(blacklist) != 0:\n",
    "    print(\"WARNING: measurements with large deviations (>10%) detected. Corresponding observations will be removed!\")\n",
    "    print(\"Number of blacklisted configurations: \", len(blacklist))\n",
    "    # remove measurements where the deviations are too big\n",
    "    data_shape = data.shape\n",
    "    for config in blacklist:\n",
    "        data = data.loc[~(data[list(config)] == pd.Series(config)).all(axis=1)]\n",
    "    print(f\"A total of {data_shape[0]-data.shape[0]} rows have been removed.\")\n",
    "else:\n",
    "    print(\"No bad measurements found!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions\n",
    "## Load configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load config\n",
    "with open('../model_fitting_and_estimation_config.yaml', \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute full architecture and channel-wise predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_architecture_preds = pd.DataFrame(columns=[\"y\", \"y_hat\", \"batch_size\", \"architecture\"])\n",
    "channel_wise_preds = pd.DataFrame(columns=[\"y\", \"y_hat\", \"batch_size\", \"architecture\", \"module\", \"layer_idx\", \"sanity\"])\n",
    "data = df\n",
    "\n",
    "for a_name, architecture in architectures_dict.items():\n",
    "    # get data for the model\n",
    "    data_a = data.loc[data['architecture'] == a_name].copy()\n",
    "    data_a.reset_index(drop=True)\n",
    "\n",
    "    # compute the predictions for the entire model\n",
    "    # for all batch-sizes that were sampled\n",
    "    for batch_size in data_a.batch_size.unique():\n",
    "        # parse model structure and return list of channels\n",
    "        # TODO: fuck why do I need to pass batch_size so early, that's dumb really dumb\n",
    "        total_predicted_energy, channel_wise_energies = compute_energy_estimate(architecture=architecture,\n",
    "                                                                                batch_size=batch_size,\n",
    "                                                                                config=config[\"model_configurations\"])\n",
    "        # sub-select corresponding to batch_size\n",
    "        data_a_batch_size = data_a.loc[data_a.batch_size == batch_size]\n",
    "        # sub-select the measurement that corresponds to the entire row\n",
    "        a_measurement = data_a_batch_size.loc[data_a_batch_size.layer_idx == 0].copy()\n",
    "\n",
    "        new_row = {\n",
    "            \"y\": [a_measurement.cpu_energy.item()],\n",
    "            \"y_hat\": [total_predicted_energy],\n",
    "            \"batch_size\": [batch_size],\n",
    "            \"architecture\": [a_name]\n",
    "        }\n",
    "        full_architecture_preds = pd.concat([full_architecture_preds, pd.DataFrame(new_row)], ignore_index=True)\n",
    "\n",
    "        # parse and combine local energy predictions with measurements\n",
    "        # sub-select from dataframe\n",
    "        data_a_compwise = data_a_batch_size.loc[data_a_batch_size.layer_idx != 0]\n",
    "        if len(data_a_compwise) != 0:\n",
    "            # remove Dropout and Adaptive Pooling layers\n",
    "            # data_a_compwise = data_a_compwise.loc[~((data_a_compwise.module == \"AdaptiveAvgPool2d\") | (data_a_compwise.module == \"Dropout\"))]\n",
    "            data_a_compwise.reset_index(inplace=True, drop=True)\n",
    "            for idx, row in data_a_compwise.iterrows():\n",
    "                new_row = {\n",
    "                    \"y\": [row.cpu_energy],\n",
    "                    \"y_hat\": [channel_wise_energies[idx][1]],\n",
    "                    \"batch_size\": [batch_size],\n",
    "                    \"architecture\": [a_name],\n",
    "                    \"module\": [row.module],\n",
    "                    \"layer_idx\": [row.layer_idx],\n",
    "                    \"sanity\": [channel_wise_energies[idx][0]]\n",
    "                }\n",
    "                channel_wise_preds = pd.concat([channel_wise_preds, pd.DataFrame(new_row)], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze Results\n",
    "# Complete architecture estimates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "r2 = r2_score(full_architecture_preds.y, full_architecture_preds.y_hat)\n",
    "print(\"Overall total R2-Score: \", r2)\n",
    "for a_name, architecture in architectures_dict.items():\n",
    "    # sub-select predictions by model\n",
    "    full_prediction_single_model = full_architecture_preds.loc[full_architecture_preds.architecture == a_name]\n",
    "    print(\"-----------\")\n",
    "    percent_deviation = abs(full_prediction_single_model.y_hat - full_prediction_single_model.y) / full_prediction_single_model.y\n",
    "    print(a_name)\n",
    "    print(f\"R2-Score: \",r2_score(full_prediction_single_model.y, full_prediction_single_model.y_hat))\n",
    "    print(f\"Avg-Abs-%-Deviation: {percent_deviation.mean():.2%}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "g = sns.scatterplot(data=full_architecture_preds, x=\"y\", y=\"y_hat\", hue=\"architecture\",palette=[\"#4C72B0\",\"#DD8452\",\"#55A868\",\"#C44E52\"])\n",
    "min_x = min(min(full_architecture_preds.y), min(full_architecture_preds.y_hat))\n",
    "max_x = max(max(full_architecture_preds.y), max(full_architecture_preds.y_hat))\n",
    "g.plot([min_x, max_x], [min_x, max_x], transform=g.transData, linestyle=\"--\", color=\"#f032e6\")\n",
    "g.set_xlabel(\"Ground Truth\")\n",
    "g.set_ylabel(\"Predictions\")\n",
    "custom_lines = [\n",
    "                plt.Line2D([0], [0], color=\"#4C72B0\", lw=2),\n",
    "                plt.Line2D([0], [0], color=\"#DD8452\", lw=2),\n",
    "                plt.Line2D([0], [0], color=\"#55A868\", lw=2),\n",
    "                plt.Line2D([0], [0], color=\"#C44E52\", lw=2),\n",
    "]\n",
    "custom_lines2 = [plt.Line2D([0], [0], color=\"#f032e6\", lw=2, linestyle=\"--\")]\n",
    "legend2 = g.legend(custom_lines2, [\"ideal\"], bbox_to_anchor=(0.29, 0.980), loc='upper right', borderaxespad=0.)\n",
    "g.add_artist(legend2)\n",
    "plt.legend(custom_lines, ['AlexNet', 'VGG13', 'VGG11', 'VGG16'], title=\"architecture\", loc=\"best\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Channel-wise estimates\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for channel_type in channel_wise_preds.module.unique():\n",
    "    channel_preds = channel_wise_preds.loc[channel_wise_preds.module == channel_type]\n",
    "    percent_deviation = abs(channel_preds.y_hat - channel_preds.y) / channel_preds.y\n",
    "    print(channel_type)\n",
    "    print(f\"R2-Score: \", r2_score(channel_preds.y, channel_preds.y_hat))\n",
    "    print(f\"Avg-Abs-%-Deviation: {percent_deviation.mean():.2%}\")\n",
    "    plt.figure(figsize=(10,5))\n",
    "    g = sns.scatterplot(data=channel_preds, x=\"y\", y=\"y_hat\", hue=\"architecture\", palette=[\"#4C72B0\", \"#DD8452\", \"#55A868\", \"#C44E52\"])\n",
    "    min_x = min(min(channel_preds.y), min(channel_preds.y_hat))\n",
    "    max_x = max(max(channel_preds.y), max(channel_preds.y_hat))\n",
    "    g.plot([min_x, max_x], [min_x, max_x], transform=g.transData, linestyle=\"--\", color=\"#f032e6\")\n",
    "    g.set_xlabel(\"Ground Truth\")\n",
    "    g.set_ylabel(\"Predictions\")\n",
    "    custom_lines = [\n",
    "                    plt.Line2D([0], [0], color=\"#4C72B0\", lw=2),\n",
    "                    plt.Line2D([0], [0], color=\"#DD8452\", lw=2),\n",
    "                    plt.Line2D([0], [0], color=\"#55A868\", lw=2),\n",
    "                    plt.Line2D([0], [0], color=\"#C44E52\", lw=2),\n",
    "    ]\n",
    "    custom_lines2 = [plt.Line2D([0], [0], color=\"#f032e6\", lw=2, linestyle=\"--\")]\n",
    "    legend2 = g.legend(custom_lines2, [\"ideal\"], bbox_to_anchor=(0.29, 0.980), loc='upper right', borderaxespad=0.)\n",
    "    g.add_artist(legend2)\n",
    "    plt.legend(custom_lines, ['AlexNet', 'VGG13', 'VGG11', 'VGG16'], title=\"architecture\", loc=\"best\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
